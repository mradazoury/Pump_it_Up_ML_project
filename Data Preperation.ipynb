{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Packages & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, ShuffleSplit, validation_curve, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, RobustScaler, LabelEncoder, scale, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE,SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import math as m\n",
    "import requests\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%aimport data_prep\n",
    "%run data_prep.py\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"training_set_values.csv\")\n",
    "train_labels = pd.read_csv(\"training_set_labels.csv\")\n",
    "test_data = pd.read_csv(\"test_set_values.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`status_group` added to train_data \n",
      "\n",
      "['id', 'recorded_by', 'scheme_name', 'ward', 'wpt_name', 'subvillage', 'waterpoint_type_group'] removed from dataset \n",
      "\n",
      "['id', 'recorded_by', 'scheme_name', 'ward', 'wpt_name', 'subvillage', 'waterpoint_type_group'] removed from dataset \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join Labels with Train data\n",
    "train_data = addLabelToTrainData(train_data, train_labels)\n",
    "\n",
    "#Drop id and recorded from train dataset \n",
    "train_data = prepareCols(train_data)\n",
    "test_data = prepareCols(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortlisting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortlist Trainning set columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`installer` shortlisted to {'Commu', 'DANIDA', 'DWE', 'Government', 'RWE', 'other'} only \n",
      "\n",
      "`funder` shortlisted to {'Government Of Tanzania','Danida','Hesawa','Rwssp','World Bank','Kkkt','World Vision','Unicef','Tasaf','District Council', 'other'} only \n",
      "\n",
      "`lga` shortlisted to {'Njombe','Arusha Rural','Moshi Rural','Bariadi','Rungwe','Kilosa','Kasulu','Mbozi','Meru','Bagamoyo', 'other'} only \n",
      "\n",
      "`extraction_type` shortlisted to {'gravity','nira/tanira','submersible','swn 80','mono','india mark ii','afridev','ksb', 'other'} only \n",
      "\n",
      "`scheme_management` shortlisted to {'VWC','WUG','Water authority','WUA','Water Board','Parastatal','Private operator','Company', 'other'} only \n",
      "\n",
      "`region_code` shortlisted to {11,17,12,3,5,18,19,2,16,10,4,1,13,14,20, 'other'} only \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_transformed = train_data.copy()\n",
    "\n",
    "\n",
    "# installer\n",
    "train_data_transformed = shortlist_installer(train_data_transformed)\n",
    "\n",
    "# funder\n",
    "train_data_transformed = shortlist_funder(train_data_transformed)\n",
    "\n",
    "# lga\n",
    "train_data_transformed = shortlist_lga(train_data_transformed)\n",
    "\n",
    "# extraction_type\n",
    "train_data_transformed = shortlist_extraction_type(train_data_transformed)\n",
    "\n",
    "#scheme_management\n",
    "train_data_transformed = shortlist_scheme_management(train_data_transformed)\n",
    "\n",
    "#region_code\n",
    "train_data_transformed = shortlist_region_code(train_data_transformed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortlist Test set columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`installer` shortlisted to {'Commu', 'DANIDA', 'DWE', 'Government', 'RWE', 'other'} only \n",
      "\n",
      "`funder` shortlisted to {'Government Of Tanzania','Danida','Hesawa','Rwssp','World Bank','Kkkt','World Vision','Unicef','Tasaf','District Council', 'other'} only \n",
      "\n",
      "`lga` shortlisted to {'Njombe','Arusha Rural','Moshi Rural','Bariadi','Rungwe','Kilosa','Kasulu','Mbozi','Meru','Bagamoyo', 'other'} only \n",
      "\n",
      "`extraction_type` shortlisted to {'gravity','nira/tanira','submersible','swn 80','mono','india mark ii','afridev','ksb', 'other'} only \n",
      "\n",
      "`scheme_management` shortlisted to {'VWC','WUG','Water authority','WUA','Water Board','Parastatal','Private operator','Company', 'other'} only \n",
      "\n",
      "`region_code` shortlisted to {11,17,12,3,5,18,19,2,16,10,4,1,13,14,20, 'other'} only \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_transformed = test_data.copy()\n",
    "\n",
    "\n",
    "# installer\n",
    "test_data_transformed = shortlist_installer(test_data_transformed)\n",
    "\n",
    "# funder\n",
    "test_data_transformed = shortlist_funder(test_data_transformed)\n",
    "\n",
    "# lga\n",
    "test_data_transformed = shortlist_lga(test_data_transformed)\n",
    "\n",
    "# extraction_type\n",
    "test_data_transformed = shortlist_extraction_type(test_data_transformed)\n",
    "\n",
    "#scheme_management\n",
    "test_data_transformed = shortlist_scheme_management(test_data_transformed)\n",
    "\n",
    "#region_code\n",
    "test_data_transformed = shortlist_region_code(test_data_transformed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f1a22550cfdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Impute Latitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpute_lat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Impute Longitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ML_project/data_prep.py\u001b[0m in \u001b[0;36mimpute_lat\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    297\u001b[0m                       'float16', 'float32', 'float64']\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"subvillage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lga\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"district_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"region\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"basin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# Impute 0 for regions of 'Dodoma','Kagera','Mbeya','Tabora' this regions 0 ar actually missing values\n",
    "train_data_transformed = amount_tsh_impute_regions(train_data_transformed)\n",
    "\n",
    "#Impute Latitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "train_data_transformed = impute_lat(train_data_transformed)\n",
    "\n",
    "#Impute Longitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "train_data_transformed = impute_long(train_data_transformed)\n",
    "\n",
    "#Impute Population by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "train_data_transformed = impute_pop(train_data_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 0 for regions of 'Dodoma','Kagera','Mbeya','Tabora' this regions 0 ar actually missing values\n",
    "test_data_transformed = amount_tsh_impute_regions(test_data_transformed)\n",
    "\n",
    "#Impute Latitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "test_data_transformed = impute_lat(test_data_transformed)\n",
    "\n",
    "#Impute Longitude by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "test_data_transformed = impute_long(test_data_transformed)\n",
    "\n",
    "#Impute Population by the mean of the geographical areas (increasing order \"subvillage\", \"ward\", \"lga\", \"district_code\", \"region\", \"basin\")\n",
    "test_data_transformed = impute_pop(test_data_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Outside Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = density(train_data)\n",
    "test_data = density(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions\n",
    "#### Convert `construction_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction_year - converts it to years elapsed (AKA age) -- (zeroes ignored)\n",
    "train_data_transformed = convert_construction_year(train_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction_year - converts it to years elapsed (AKA age) -- (zeroes ignored)\n",
    "test_data_transformed = convert_construction_year(test_data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^^^^^^^^^^^^^^^^^^^^^^^^^ What to do with zeros here ? (summary  ↓ ) -- currently imputing with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes =    train_data_transformed['age'][train_data_transformed['age'] == 0].count() / train_data_transformed['age'].count()\n",
    "mean = np.mean(train_data_transformed['age'][train_data_transformed['age'] != 0])\n",
    "median = np.median(train_data_transformed['age'][train_data_transformed['age'] != 0])\n",
    "\n",
    "print(\"% of zeroes: {}\".format(zeroes))\n",
    "\n",
    "print(\"mean: {}\".format(mean))\n",
    "\n",
    "\n",
    "print(\"median: {}\".format(median))\n",
    "\n",
    "# impute with median\n",
    "train_data_transformed[train_data_transformed['age'] == 0][[\"age\"]] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply same process for Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert `date_recorded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = convert_date_recorded(train_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_transformed = convert_date_recorded(test_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mapping long and lat to city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = bin_feature(train_data_transformed, \"days_since_recoreded\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_transformed = bin_feature(test_data_transformed, \"days_since_recoreded\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['amount_tsh', 'gps_height', 'num_private', 'population']\n",
    "other = ['longitude', 'latitude', \"status_code\"]\n",
    "categorical = list(set(train_data_transformed.columns) - set(numerical) - set(other))\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp['latitude'] = train_temp[\"latitude\"].astype('category') \n",
    "train_temp['longitude'] = train_temp[\"longitude\"].astype('category') \n",
    "\n",
    "test_temp['latitude'] = test_temp[\"latitude\"].astype('category') \n",
    "test_temp['longitude'] = test_temp[\"longitude\"].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_skewness(test_temp)\n",
    "feature_skewness(train_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp = fix_skewness(test_temp)\n",
    "train_temp = fix_skewness(train_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.figure(figsize=(20,10))\n",
    "sns.heatmap(train_data_transformed.corr(),cbar=True,fmt =' .2f', annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot Encode & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_transformed.status_group.replace(['functional', 'non functional','functional needs repair'], [1, 2, 3], inplace=True)\n",
    "labels_prepared = train_data_transformed[[\"status_group\"]]\n",
    "train_data_transformed = train_data_transformed.drop(columns=[\"status_group\"])\n",
    "#train_data_transformed = onehot_encode(train_data_transformed)\n",
    "train_data_transformed.to_csv(\"train_data_prepared.csv\")\n",
    "labels_prepared.to_csv(\"labels_prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_transformed.to_csv(\"test_data_prepared.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
